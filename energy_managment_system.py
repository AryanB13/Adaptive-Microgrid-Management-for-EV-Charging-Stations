# -*- coding: utf-8 -*-
"""Energy_Managment_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ukx6Gj519cQDUlEsPK-9fvDO8Xg-IQ_i
"""

from google.colab import files
import scipy.io

# Step 1: Upload the MATLAB file to Google Colab
uploaded = files.upload()

# Step 2: Get the file name
file_name = list(uploaded.keys())[0]

# Step 3: Load variable information from the MATLAB file
mat_variables = scipy.io.whosmat(file_name)

# Print the variables and their types
print("Variables in the MATLAB file:")
for variable in mat_variables:
    print(f"Variable name: {variable[0]}, Type: {variable[1]}")

# Step 4: Load data from the MATLAB file
mat_data = scipy.io.loadmat(file_name)

# You can now access the 'EV_load' variable in the 'mat_data' dictionary.
# Access the entire array of EV_load
EV_load_mat= mat_data['EV_load']
EV_load_ok= EV_load_mat.tolist()
EV_load = [item for sublist in EV_load_ok for item in sublist]

from google.colab import files
import scipy.io

# Step 1: Upload the MATLAB file to Google Colab
uploaded = files.upload()

# Step 2: Get the file name
file_name = list(uploaded.keys())[0]

# Step 3: Load variable information from the MATLAB file
mat_variables = scipy.io.whosmat(file_name)

# Print the variables and their types
print("Variables in the MATLAB file:")
for variable in mat_variables:
    print(f"Variable name: {variable[0]}, Type: {variable[1]}")

# Step 4: Load data from the MATLAB file
mat_data = scipy.io.loadmat(file_name)

# You can now access the 'EV_load' variable in the 'mat_data' dictionary.
# Access the entire array of EV_load
PV_mat= mat_data['PV_data']
PV_ok= PV_mat.tolist()
PV = [item for sublist in PV_ok for item in sublist]

from google.colab import files
import scipy.io

# Step 1: Upload the MATLAB file to Google Colab
uploaded = files.upload()

# Step 2: Get the file name
file_name = list(uploaded.keys())[0]

# Step 3: Load variable information from the MATLAB file
mat_variables = scipy.io.whosmat(file_name)

# Print the variables and their types
print("Variables in the MATLAB file:")
for variable in mat_variables:
    print(f"Variable name: {variable[0]}, Type: {variable[1]}")

# Step 4: Load data from the MATLAB file
mat_data = scipy.io.loadmat(file_name)

# You can now access the 'EV_load' variable in the 'mat_data' dictionary.
# Access the entire array of EV_load
Price_mat= mat_data['cost96']
Price_ok= Price_mat.tolist()
Price = [item for sublist in Price_ok for item in sublist]

import numpy as np
EV_load=np.array(EV_load)
PV=np.array(PV)
Price=np.array(Price)

print(len(EV_load))
print(len(PV))
print(len(Price))

P_Grid_Graph=[]
Batt_soc_Graph=[]
Batt_Profile_Graph=[]
Batt_power_graph=[]



import random
import gym
from gym import spaces
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

# Parameters
GRID_LIMIT = 50000
BATT_LIMIT = 20000
PV_LIMIT = 30000
P_LOAD = 80000
BATT_CAP = 100000
STATE_SIZE = 4
ACTION_SIZE = 21
MEMORY_SIZE = 2000
GAMMA = 0.95
EPSILON_INIT = 1.0
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
LEARNING_RATE = 0.001
BATCH_SIZE = 32
NUM_EPISODES = 1000

def batt_reward(Power, SOC):
    Pbf = ((SOC - 0.5) / ((0.8 - 0.2) / 2)) * BATT_LIMIT
    return -0.5 * (Power - Pbf) ** 2

def Grid_reward(Power_grid):
    P_gr=(0.5*(Power_grid-0)**2)
    return -1*(P_gr)


class MicrogridEnv(gym.Env):
    def __init__(self):
        super(MicrogridEnv, self).__init__()

        # Defining action [batt]
        self.action_space = spaces.Discrete(21)

        # States [grid, price, pv, pbatt, Ebatt, load]
        self.observation_space = spaces.Box(low=np.array([-GRID_LIMIT, 2, 0, -BATT_LIMIT, 0.2 * BATT_CAP, 0]),
                                     high=np.array([GRID_LIMIT, 10, PV_LIMIT, 0.8 * BATT_CAP, P_LOAD, 10]),
                                     dtype=np.int32)


        # Microgrid parameters
        self.initial_load = 20000
        self.initial_pv = 0
        self.initial_soc = 0.5 * BATT_CAP
        self.bank_capacity = BATT_CAP
        self.initial_bank_charge = 0.5 * BATT_CAP
        self.initial_grid_price = 4
        self.charge_rate = 0.35

        # Initializing state
        self.current_load = self.initial_load
        self.current_pv = self.initial_pv
        self.current_soc = self.initial_soc
        self.current_price = self.initial_grid_price
        self.current_charge = self.initial_bank_charge
        self.current_time_step = 0
        self.PV_load= np.array(EV_load)# Placeholder list of zeros


        # Placeholder values for PV mean and std
        self.PV_mean =np.array(PV)     # Placeholder list of zeros


    def reset(self):
        # Resetting the state and returning it
        self.current_load = self.initial_load
        self.current_pv = self.initial_pv
        self.current_soc = self.initial_soc
        self.current_price = self.initial_grid_price
        self.current_charge = self.initial_bank_charge
        self.current_time_step = 0
        return np.array([self.current_load, self.current_pv, self.current_soc, self.current_price])

    def step(self, action):

        action_val=(action*2000)-20000
       # Batt_Profile_Graph.append(action_val)

        Power_grid = self.current_load - self.current_pv - action_val
        reward_grid = Grid_reward(Power_grid)
        reward_batt = batt_reward(action_val, self.current_soc)
        reward = reward_grid + reward_batt
       # P_Grid_Graph.append(Power_grid)

        # State transitions according to the distribution associated with power
        t = self.current_time_step
        self.current_load = EV_load[t]
        self.current_load = min(P_LOAD, self.current_load)
        self.current_load = max(0, self.current_load)

        self.current_pv = PV[t]
        self.current_pv = min(PV_LIMIT, self.current_pv)
        self.current_pv = max(0, self.current_pv)

        self.current_price = Price[t]
        self.current_price = min(8, self.current_price)
        self.current_price = max(0, self.current_price)

        # Maintaining battery charge
        self.current_charge = self.current_charge - (self.charge_rate) * action_val
        self.current_soc = (self.current_charge / self.bank_capacity)
        self.current_soc = min(0.8, self.current_soc)
        self.current_soc = max(0.2, self.current_soc)
        # Batt_soc_Graph.append(self.current_soc)
        # Batt_power_graph.append(self.current_charge)

        # Increment time step
        self.current_time_step = self.current_time_step+1

        # Episode is done or not
        done = (self.current_time_step >= 96)
        return np.array([self.current_load, self.current_pv, self.current_soc, self.current_price]), reward, done, {}

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=MEMORY_SIZE)
        self.gamma = GAMMA
        self.epsilon = EPSILON_INIT
        self.epsilon_min = EPSILON_MIN
        self.epsilon_decay = EPSILON_DECAY
        self.learning_rate = LEARNING_RATE
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Flatten(input_shape=(self.state_size,)))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Initialize the environment
env = MicrogridEnv()

# Initialize the DQN agent
agent = DQNAgent(STATE_SIZE, ACTION_SIZE)

# Training loop
scores = []
global_action=[]
ns_global=[]

for episode in range(200):
    state = env.reset()
    state = np.zeros([1, STATE_SIZE])

    total_reward = 0
    done = False
    action_ok=[]
    ns_ok=[]
    while not done:
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, STATE_SIZE])
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward
        action_ok.append(action)
        ns_ok.append(next_state)
    scores.append(total_reward)
    global_action.append(action_ok)
    ns_global.append(ns_ok)
    print(f"Episode: {episode + 1}, Total Reward: {total_reward}")
    if len(agent.memory) > BATCH_SIZE:
        agent.replay(BATCH_SIZE)

# Plotting scores
max_index = scores.index(max(scores))
scores=sorted(scores)
plt.plot(scores)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress')
plt.show()


action_policy= [20, 8, 15, 7, 8, 1, 8, 5, 8, 11, 8, 8, 10, 11, 19, 8, 11, 15, 8, 4, 18, 3, 14, 9, 8, 20, 18, 3, 5, 4, 8, 1, 19, 18, 12, 9, 8, 0, 5, 0, 18, 8, 15, 9, 8, 8, 17, 8, 6, 8, 6, 9, 8, 10, 5, 13, 4, 13, 8, 1, 8, 8, 7, 7, 18, 12, 5, 15, 8, 8, 8, 1, 8, 0, 2, 19, 15, 9, 2, 8, 17, 8, 14, 7, 13, 8, 10, 8, 20, 8, 13, 8, 13, 8, 18, 8]

for i in range(len(action_policy)):
  action_policy[i]=(action_policy[i]*2000)-20000

plt.plot(action_policy)
plt.xlabel('timestep')
plt.ylabel('battery_power')
plt.title('battery_profile')
plt.show()

import random
import gym
from gym import spaces
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

# Parameters
GRID_LIMIT = 50000
BATT_LIMIT = 20000
PV_LIMIT = 30000
P_LOAD = 80000
BATT_CAP = 100000
STATE_SIZE = 4
ACTION_SIZE = 21
MEMORY_SIZE = 2000
GAMMA = 0.95
EPSILON_INIT = 1.0
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
LEARNING_RATE = 0.001
BATCH_SIZE = 32
NUM_EPISODES = 1000

def batt_reward(Power, SOC):
    Pbf = ((SOC - 0.5) / ((0.8 - 0.2) / 2)) * BATT_LIMIT
    return -0.5 * (Power - Pbf) ** 2

def Grid_reward(Power_grid):
    P_gr=(0.5*(Power_grid-0)**2)
    return -1*(P_gr)


class MicrogridEnv(gym.Env):
    def __init__(self):
        super(MicrogridEnv, self).__init__()

        # Defining action [batt]
        self.action_space = spaces.Discrete(21)

        # States [grid, price, pv, pbatt, Ebatt, load]
        self.observation_space = spaces.Box(low=np.array([-GRID_LIMIT, 2, 0, -BATT_LIMIT, 0.2 * BATT_CAP, 0]),
                                     high=np.array([GRID_LIMIT, 10, PV_LIMIT, 0.8 * BATT_CAP, P_LOAD, 10]),
                                     dtype=np.int32)


        # Microgrid parameters
        self.initial_load = 20000
        self.initial_pv = 0
        self.initial_soc = 0.5 * BATT_CAP
        self.bank_capacity = BATT_CAP
        self.initial_bank_charge = 0.5 * BATT_CAP
        self.initial_grid_price = 4
        self.charge_rate = 0.35

        # Initializing state
        self.current_load = self.initial_load
        self.current_pv = self.initial_pv
        self.current_soc = self.initial_soc
        self.current_price = self.initial_grid_price
        self.current_charge = self.initial_bank_charge
        self.current_time_step = 0
        self.PV_load= np.array(EV_load)# Placeholder list of zeros


        # Placeholder values for PV mean and std
        self.PV_mean =np.array(PV)     # Placeholder list of zeros


    def reset(self):
        # Resetting the state and returning it
        self.current_load = self.initial_load
        self.current_pv = self.initial_pv
        self.current_soc = self.initial_soc
        self.current_price = self.initial_grid_price
        self.current_charge = self.initial_bank_charge
        self.current_time_step = 0
        return np.array([self.current_load, self.current_pv, self.current_soc, self.current_price])

    def step(self, action):

        action_val=(action*2000)-20000
        Batt_Profile_Graph.append(action_val)

        Power_grid = self.current_load - self.current_pv - action_val
        reward_grid = Grid_reward(Power_grid)
        reward_batt = batt_reward(action_val, self.current_soc)
        reward = reward_grid + reward_batt
        P_Grid_Graph.append(Power_grid)

        # State transitions according to the distribution associated with power
        t = self.current_time_step
        self.current_load = EV_load[t]
        self.current_load = min(P_LOAD, self.current_load)
        self.current_load = max(0, self.current_load)

        self.current_pv = PV[t]
        self.current_pv = min(PV_LIMIT, self.current_pv)
        self.current_pv = max(0, self.current_pv)

        self.current_price = Price[t]
        self.current_price = min(8, self.current_price)
        self.current_price = max(0, self.current_price)

        # Maintaining battery charge
        self.current_charge = self.current_charge - (self.charge_rate) * action_val
        self.current_soc = (self.current_charge / self.bank_capacity)
        self.current_soc = min(0.8, self.current_soc)
        self.current_soc = max(0.2, self.current_soc)
        Batt_soc_Graph.append(self.current_soc)
        # Batt_power_graph.append(self.current_charge)

        # Increment time step
        self.current_time_step = self.current_time_step+1

        # Episode is done or not
        done = (self.current_time_step >= 96)
        return np.array([self.current_load, self.current_pv, self.current_soc, self.current_price]), reward, done, {}

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=MEMORY_SIZE)
        self.gamma = GAMMA
        self.epsilon = EPSILON_INIT
        self.epsilon_min = EPSILON_MIN
        self.epsilon_decay = EPSILON_DECAY
        self.learning_rate = LEARNING_RATE
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Flatten(input_shape=(self.state_size,)))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Initialize the environment
env = MicrogridEnv()

# Initialize the DQN agent
agent = DQNAgent(STATE_SIZE, ACTION_SIZE)

# Training loop
scores = []
# global_action=[]
# ns_global=[]

for episode in range(5):
    state = env.reset()
    state = np.zeros([1, STATE_SIZE])
    ct=0
    total_reward = 0
    done = False
    # action_ok=[]
    # ns_ok=[]
    norm=1000
    while not done:
        action = action_policy[ct]
        ct=ct+1
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, STATE_SIZE])
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward
        # action_ok.append(action)
        # ns_ok.append(next_state)
    scores.append(total_reward)
    # global_action.append(action_ok)
    # ns_global.append(ns_ok)
    print(f"Episode: {episode + 1}, Total Reward: {total_reward}")
    if len(agent.memory) > BATCH_SIZE:
        agent.replay(BATCH_SIZE)

# Plotting scores
max_index = scores.index(max(scores))
new_scores=sorted(scores)
plt.plot(scores)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Training Progress')
plt.show()

next_state_array = np.array(ns_global)
cur_soc=[]
# Convert to list
next_state_list = next_state_array.tolist()
cur_soc=[]
print(next_state_list[0][1][0])
for i in range(96):
  cur_soc.append(next_state_list[0][i][0][2])

print(cur_soc)

# import matplotlib.pyplot as plt
# plt.plot(range(len(scores)),scores)
# plt.xlim(0, len(scores)*0.01)
# plt.xlabel('global time step')
# plt.ylabel('batt_prof')
# plt.title(' for all episodes combined')
# print(scores)

plt.plot(range(len(Batt_soc_Graph)),Batt_soc_Graph)
plt.xlim(0, len(Batt_soc_Graph)*0.001)
plt.xlabel('global time step')
plt.ylabel('batt soc')
plt.title('batt soc for all episodes combined')
plt.show()

plt.plot(range(len(P_Grid_Graph)),P_Grid_Graph)
plt.xlim(0, len(P_Grid_Graph)*0.001)
plt.xlabel('global time step')
plt.ylabel('grid power')
plt.title('grid_power for all episodes combined')
plt.show()


EV_load



plt.plot(EV_load)
plt.xlabel('time')
plt.ylabel('EV_load')
plt.title('EV_load for different timesteps')
plt.show()

plt.plot(PV)
plt.xlabel('time')
plt.ylabel('PV')
plt.title('PV value for different timesteps')
plt.show()

